{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from IPython.display import display as show_this\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import sys\n",
    "import os, fnmatch\n",
    "sys.path.append('waveglow/')\n",
    "\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.io.wavfile import write\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2, load_model\n",
    "from waveglow.denoiser import Denoiser\n",
    "from layers import TacotronSTFT\n",
    "from data_utils import TextMelLoader, TextMelCollate\n",
    "from text import cmudict, text_to_sequence\n",
    "from mellotron_utils import get_data_from_musicxml\n",
    "from mcd_wrapper import _calculate_mcd\n",
    "from interpolate_rhythm import Cubic_Solver\n",
    "from train_step import Train_Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panner(signal, angle):\n",
    "    angle = np.radians(angle)\n",
    "    left = np.sqrt(2)/2.0 * (np.cos(angle) - np.sin(angle)) * signal\n",
    "    right = np.sqrt(2)/2.0 * (np.cos(angle) + np.sin(angle)) * signal\n",
    "    return np.dstack((left, right))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_from_alignment(input):\n",
    "    src = input.transpose(1,2).squeeze()\n",
    "    line_vals = torch.argmax(src, dim=1).to(dtype=torch.float)\n",
    "    maxv = torch.max(src)\n",
    "    solver = Cubic_Solver()\n",
    "    x = torch.FloatTensor(range(src.size(0)))\n",
    "    # print(x.shape, line_vals.shape)\n",
    "    solver.train(x, line_vals,100000)\n",
    "    out = torch.zeros(src.size())\n",
    "    out2 = torch.zeros(src.size())\n",
    "    with torch.no_grad():\n",
    "          k=solver.calc(x).floor().to(dtype=torch.int)\n",
    "    #print(k)\n",
    "    sz = src.size(1)-1\n",
    "    for i in range(src.size(0)):\n",
    "        k = solver.calc(torch.FloatTensor([i])).to(dtype=torch.int).item()#  line_vals[i]\n",
    "        k = 0 if k < 0 else sz if k + 1 > sz else k\n",
    "        # print(i,k)\n",
    "        out[i][k] = maxv\n",
    "        out2[i][line_vals[i].int().item()] = maxv\n",
    "    return torch.unsqueeze(out, dim=1), torch.unsqueeze(out2, dim=1), line_vals\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel_f0_alignment(mel_source, mel_outputs_postnet, f0s, alignments, mel_val=0, figsize=(16, 16)):\n",
    "    fig, axes = plt.subplots(4, 1, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    axes[0].imshow(mel_source, aspect='auto', origin='bottom', interpolation='none')\n",
    "    axes[1].imshow(mel_outputs_postnet, aspect='auto', origin='bottom', interpolation='none')\n",
    "    axes[2].scatter(range(len(f0s)), f0s, alpha=0.5, color='red', marker='.', s=1)\n",
    "    axes[2].set_xlim(0, len(f0s))\n",
    "    axes[3].imshow(alignments, aspect='auto', origin='bottom', interpolation='none')\n",
    "    axes[0].set_title(\"Source Mel\")\n",
    "    axes[1].set_title(\"Predicted Mel\")\n",
    "    axes[2].set_title(\"Source pitch contour\")\n",
    "    axes[3].set_title(\"Source rhythm {:.6f}\".format(mel_val))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mels(mel_source, mel_outputs_postnet, figsize=(16, 8)):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    axes[0].imshow(mel_source, aspect='auto', origin='bottom', interpolation='none')\n",
    "    axes[0].set_title(\"Source Mel\")\n",
    "    axes[1].imshow(mel_outputs_postnet, aspect='auto', origin='bottom', interpolation='none')\n",
    "    axes[1].set_title(\"Predicted Mel\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignment(alignments, title=\"rhythm\", figsize=(16, 4)):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=figsize)\n",
    "    axes.imshow(alignments, aspect='auto', origin='bottom', interpolation='none')\n",
    "    axes.set_title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mel(path):\n",
    "    audio, sampling_rate = librosa.core.load(path, sr=hparams.sampling_rate)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    if sampling_rate != hparams.sampling_rate:\n",
    "        raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    melspec = melspec \n",
    "    if torch.cuda.is_available():\n",
    "        melspec = melspec.cuda()\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_name(mellotron_id):\n",
    "    global speakers\n",
    "    item = speakers.query(\"MELLOTRON_ID == {}\".format(mellotron_id))\n",
    "    return item['NAME'].values[0],item['SEX'].values[0],item['ID'].values[0]\n",
    "\n",
    "def get_libriTTS_sample(speaker_id):\n",
    "    start_path = \"/path_to_libritts/{}\".format(speaker_id)\n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        for name in files:\n",
    "            if fnmatch.fnmatch(name, \"*.wav\"):\n",
    "                return os.path.join(root, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = create_hparams()\n",
    "hparams.shuffle_data=False\n",
    "hparams.p_attention_dropout=0.0\n",
    "hparams.p_decoder_dropout=0.0\n",
    "hparams.p_teacher_forcing=1.1\n",
    "\n",
    "\n",
    "sample_writes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_path = \"models/mellotron_libritts.pt\"\n",
    "#checkpoint_path = \"my_models/run-May04-1247/checkpoint_93000\"\n",
    "#checkpoint_path = \"my_models/run-May04-2127/checkpoint_130000\"\n",
    "#checkpoint_path = \"my_models/run-May05-2131/checkpoint_181000\"\n",
    "#checkpoint_path = \"my_models/run-May05-2131/checkpoint_236000\"\n",
    "#checkpoint_path = \"my_models/run-May08-2308/checkpoint_354500\"\n",
    "checkpoint_path = \"my_models/checkpoint_500\"\n",
    "# checkpoint_path = \"my_models/LJ_run150k.pt\"\n",
    "# \"my_models/run-Apr27-1121/checkpoint_150000\" \n",
    "# \"my_models/run-Apr25-1542/checkpoint_101500\"\n",
    "mellotron = load_model(hparams)\n",
    "if torch.cuda.is_available():\n",
    "    mellotron = mellotron.cuda()\n",
    "mellotron.eval()\n",
    "if torch.cuda.is_available():\n",
    "    mellotron.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
    "else:\n",
    "    mellotron.load_state_dict(torch.load(checkpoint_path,map_location=torch.device('cpu'))['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waveglow_path = 'models/waveglow_256channels_v4.pt'\n",
    "if torch.cuda.is_available():\n",
    "    waveglow = torch.load(waveglow_path)['model']\n",
    "else:\n",
    "    waveglow = torch.load(waveglow_path,map_location=torch.device('cpu'))['model']\n",
    "if torch.cuda.is_available():\n",
    "    waveglow = waveglow.cuda()\n",
    "waveglow.eval()\n",
    "denoiser = Denoiser(waveglow)\n",
    "if torch.cuda.is_available():\n",
    "    denoiser = denoiser.cuda()\n",
    "denoiser.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Speakers Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker_ids = TextMelLoader(\"filelists/libritts_train_clean_100_audiopath_text_sid_atleast5min_val_filelist.txt\", hparams).speaker_ids\n",
    "speaker_ids = TextMelLoader(\"filelists/libritts_train_clean_100_audiopath_text_sid_shorterthan10s_atleast5min_train_filelist.txt\", \n",
    "                            hparams).speaker_ids\n",
    "\n",
    "speakers = pd.read_csv('filelists/libritts_speakerinfo.txt', engine='python',header=None, comment=';', sep=' *\\| *', \n",
    "                       names=['ID', 'SEX', 'SUBSET', 'MINUTES', 'NAME'])\n",
    "speakers['MELLOTRON_ID'] = speakers['ID'].apply(lambda x: speaker_ids[x] if x in speaker_ids else -1)\n",
    "female_speakers = cycle(\n",
    "    speakers.query(\"SEX == 'F' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())\n",
    "male_speakers = cycle(\n",
    "    speakers.query(\"SEX == 'M' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arpabet_dict = cmudict.CMUDict('data/cmu_dictionary',speaker_ids)\n",
    "audio_paths = 'data/examples_filelist.txt'\n",
    "audio_paths = 'data/examples_filelist-II.txt'\n",
    "dataloader = TextMelLoader(audio_paths, hparams)\n",
    "datacollate = TextMelCollate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 1\n",
    "audio_path, text, sid = dataloader.audiopaths_and_text[file_idx]\n",
    "\n",
    "# get audio path, encoded text, pitch contour and mel for gst\n",
    "text_encoded = torch.LongTensor(text_to_sequence(text, hparams.text_cleaners, arpabet_dict))[None, :]\n",
    "if torch.cuda.is_available():\n",
    "    text_encoded = text_encoded.cuda()    \n",
    "pitch_contour = dataloader[file_idx][3][None]\n",
    "if torch.cuda.is_available():\n",
    "    pitch_contour = pitch_contour.cuda()\n",
    "mel = load_mel(audio_path)\n",
    "\n",
    "# load source data to obtain rhythm using tacotron 2 as a forced aligner\n",
    "x, y = mellotron.parse_batch(datacollate([dataloader[file_idx]]))\n",
    "print(x[5].item(),audio_path, text)\n",
    "zzz=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ipd.Audio(audio_path, rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer (Rhythm and Pitch Contour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attempt to condition on source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer = Train_Step(hparams,mellotron)\n",
    "\n",
    "for i in range(100):\n",
    "    # pred, loss,mellotron = Trainer.step(x,y)\n",
    "    mel_outputs, mel_outputs_postnet, gate_outputs, rhythm, deco = mellotron.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single-shot rhythm calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mellotron.eval()\n",
    "with torch.no_grad():\n",
    "    # get rhythm (alignment map) using tacotron 2#\n",
    "    mel_outputs, mel_outputs_postnet, gate_outputs, rhythm, deco = mellotron.forward(x)\n",
    "    rhythm = rhythm.permute(1, 0, 2)\n",
    "    \n",
    "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
    "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
    "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
    "                      rhythm.data.cpu().numpy()[:, 0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## titrate rhythm Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 0\n",
    "\n",
    "if zzz < 1:\n",
    "    mcd_sav = 2000\n",
    "    mcd_test = 1000.0\n",
    "while k < 200 and i < 6:\n",
    "    with torch.no_grad():\n",
    "        # get rhythm (alignment map) using tacotron 2\n",
    "        mel_outputsx, mel_outputs_postnetx, gate_outputsx, rhythmx, deco = mellotron.forward(x)\n",
    "        rhythmx = rhythmx.permute(1, 0, 2)\n",
    "        mcd_val = _calculate_mcd(x[2].data.cpu().numpy()[0],\n",
    "                                 mel_outputs_postnetx.data.cpu().numpy()[0])\n",
    "        if mcd_val < mcd_test:\n",
    "            mel_outputs, mel_outputs_postnet, gate_outputs, rhythm = mel_outputsx, mel_outputs_postnetx, gate_outputsx, rhythmx\n",
    "            mcd_test = mcd_val\n",
    "            spkr = i\n",
    "    if k % 10 == 0 :\n",
    "        print(mcd_test)\n",
    "        if mcd_sav <= mcd_test:\n",
    "            i += 1\n",
    "        mcd_sav = mcd_test\n",
    "    k += 1\n",
    "\n",
    "zzz=2\n",
    "with torch.no_grad():\n",
    "    audiox = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[:, 0]\n",
    "\n",
    "\n",
    "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
    "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
    "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
    "                      rhythm.data.cpu().numpy()[:, 0].T, mcd_test)   \n",
    "\n",
    "ipd.Audio(audiox[0].data.cpu().numpy(), rate=hparams.sampling_rate)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO THIS FIRST (after rhythm calculation) to set (or reset) initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel2=mel\n",
    "rhythm2=rhythm\n",
    "pitch2=pitch_contour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attempt to interpolate a rhythm (cubic spline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r2, raw, _ = line_from_alignment(rhythm)\n",
    "print(r2.shape)\n",
    "plot_alignment(r2.data.cpu().numpy()[:, 0].T,\"smooth rhythm\")\n",
    "plot_alignment(raw.data.cpu().numpy()[:, 0].T,\"raw\")\n",
    "if torch.cuda.is_available():\n",
    "    rhythm2 = r2.cuda()\n",
    "else:\n",
    "    rhythm2 = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate straight line rhythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rhythm.size(2)/rhythm.size(0)\n",
    "val = rhythm.max()\n",
    "r3 = torch.zeros_like(rhythm)\n",
    "for i in range(rhythm.size(0)):\n",
    "    r3[i][0][int(i*m)] = val\n",
    "plot_alignment(r3.data.cpu().numpy()[:, 0].T)\n",
    "rhythm2 = r3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply a zero style vector (mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel2 = torch.zeros(mel.shape)\n",
    "if torch.cuda.is_available():\n",
    "    mel2=mel2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## play with this to fix pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch2=pitch_contour*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Target Speaker Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = 102 # next(female_speakers) if np.random.randint(2) else next(male_speakers)\n",
    "# speaker_id=0\n",
    "name, sex, idx = get_speaker_name(speaker_id)\n",
    "sample_path = get_libriTTS_sample(idx)\n",
    "print(speaker_id, name, sex, idx, sample_path)\n",
    "speaker_id = torch.LongTensor([speaker_id])\n",
    "if torch.cuda.is_available():\n",
    "    speaker_id = speaker_id.cuda()\n",
    "\n",
    "melx, fz = dataloader.get_mel_and_f0(sample_path)\n",
    "\n",
    "ipd.Audio(sample_path, rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use target speaker's Style vector (mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel2=melx.unsqueeze(dim=0)\n",
    "if torch.cuda.is_available():\n",
    "    mel2 = mel2.cuda()\n",
    "print(speaker_id.item(),mel.shape,mel2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mellotron.eval()\n",
    "with torch.no_grad():\n",
    "    mel_outputs, mel_outputs_postnet, gate_outputs,alignments = mellotron.inference_noattention(\n",
    "        (text_encoded, mel2, speaker_id, pitch2, rhythm2))\n",
    "\n",
    "plot_mels(x[2].data.cpu().numpy()[0], mel_outputs_postnet.data.cpu().numpy()[0])\n",
    "\n",
    "mcd_val = _calculate_mcd(x[2].data.cpu().numpy()[0],\n",
    "                      mel_outputs_postnet.data.cpu().numpy()[0])\n",
    "\n",
    "print(mcd_val)\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[:, 0]\n",
    "\n",
    "\n",
    "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## titrate and generate sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd = 1000\n",
    "for n in range(3):\n",
    "    for i in range(20):\n",
    "        with torch.no_grad():\n",
    "            mel3 = i #torch.LongTensor(i)\n",
    "            mel_outputs, mel_outputs_postnetx, gate_outputs,alignments = mellotron.inference_noattention(\n",
    "                (text_encoded, mel2, speaker_id, pitch2, rhythm2))\n",
    "            mcd_val = _calculate_mcd(x[2].data.cpu().numpy()[0],\n",
    "                                     mel_outputs_postnetx.data.cpu().numpy()[0])\n",
    "            if mcd_val < mcd:\n",
    "                mcd = mcd_val\n",
    "                mel_outputs_postnet = mel_outputs_postnetx\n",
    "                alignment = alignments\n",
    "    print(mcd)\n",
    "\n",
    "plot_mels(x[2].data.cpu().numpy()[0], mel_outputs_postnet.data.cpu().numpy()[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[:, 0]\n",
    "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_writes)\n",
    "def do_save(b):\n",
    "    global sample_writes\n",
    "    audio_out = audio[0].data.cpu().numpy()\n",
    "    audio_out = audio_out / np.max(np.abs(audio_out))\n",
    "    filename = \"mellotron_sample.{}.wav\".format(sample_writes)\n",
    "    write(filename, hparams.sampling_rate, audio_out)\n",
    "    sample_writes += 1\n",
    "    print(filename +\" saved\")\n",
    "\n",
    "save_button = widgets.Button(description=\"save\")\n",
    "show_this(save_button)\n",
    "save_button.on_click(do_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singing Voice from Music Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_from_musicxml('data/haendel_hallelujah.musicxml', 132, convert_stress=True)\n",
    "panning = {'Soprano': [-60, -30], 'Alto': [-40, -10], 'Tenor': [30, 60], 'Bass': [10, 40]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_speakers_per_part = 4\n",
    "frequency_scaling = 0.4\n",
    "n_seconds = 90\n",
    "audio_stereo = np.zeros((hparams.sampling_rate*n_seconds, 2), dtype=np.float32)\n",
    "for i, (part, v) in enumerate(data.items()):\n",
    "    rhythm = data[part]['rhythm'].cuda()\n",
    "    pitch_contour = data[part]['pitch_contour'].cuda()\n",
    "    text_encoded = data[part]['text_encoded'].cuda()\n",
    "    \n",
    "    for k in range(n_speakers_per_part):\n",
    "        pan = np.random.randint(panning[part][0], panning[part][1])\n",
    "        if any(x in part.lower() for x in ('soprano', 'alto', 'female')):\n",
    "            speaker_id = torch.LongTensor([next(female_speakers)]).cuda()\n",
    "        else:\n",
    "            speaker_id = torch.LongTensor([next(male_speakers)]).cuda()\n",
    "        print(\"{} MellotronID {} pan {}\".format(part, speaker_id.item(), pan))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments_transfer = mellotron.inference_noattention(\n",
    "                (text_encoded, mel, speaker_id, pitch_contour*frequency_scaling, rhythm))\n",
    "\n",
    "            audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[0, 0]\n",
    "            audio = audio.cpu().numpy()\n",
    "            audio = panner(audio, pan)\n",
    "            audio_stereo[:audio.shape[0]] += audio            \n",
    "            write(\"{} {}.wav\".format(part, speaker_id.item()), hparams.sampling_rate, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_stereo = audio_stereo / np.max(np.abs(audio_stereo))\n",
    "write(\"audio_stereo.wav\", hparams.sampling_rate, audio_stereo)\n",
    "ipd.Audio([audio_stereo[:,0], audio_stereo[:,1]], rate=hparams.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
